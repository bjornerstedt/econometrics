---
author: "Jonas Bj√∂rnerstedt"
title: "Econometrics - Lecture 3"
subtitle: "Linear Regression with One Regressor"
date: '`r Sys.Date()`'
output:
    ioslides_presentation:
        css: slides.css
runtime: shiny
...

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(shinyPresentation = TRUE)
library(tidyverse)
library(broom)
library(readxl)
library(stargazer)
library(knitr)
library(shiny)
library(MASS)
library(png)
library(grid)
library(gridExtra)
set.seed(423)
```

## Lecture Content

Part II. Fundamentals of Regression Analysis

Chapter 4. Linear Regression with One Regressor

- Generalize the concept of _mean_
    - _Conditional mean_ $E(Y|X)$ of $Y$
    - Given $X$, what is the mean of $Y$?

## Variance and correlation 
- Relationship between different random variables
- Variance: $\sigma^2_Y = Var(Y)=E[(Y - \mu_Y)^2]$
    - Expected square distance from mean
    - Average square distance in sample
    - Standard deviation $\sigma_Y = \sqrt{Var(Y)}$
- Covariance: 
$$Cov(X, Y) = E[(X - \mu_X)(Y- \mu_y)]$$
    - Expected product of deviations from average for $X$ and $Y$
    - Average product of deviations in sample

## Correlation coefficent

- Normalize covariance with the standard deviation of $X$ and $Y$
$$\rho_{XY} = \frac{Cov(X, Y)}{\sigma_X \sigma_Y }$$
- We then have
$$-1 \le\rho_{XY}\le 1$$


## Scatterplots
- Scatterplots the sample covariance and the sample correlation
- Sample covariance and correlation
- find data cps12.dta

## Sample covariance and correlation

- The sample variance $s_{Y}$ is given by 
$$s_{Y}^2 = \frac{1}{n-1} \sum_{i=1}^n (Y_i - \bar Y)^2$$ 
- The sample covariance $s_{XY}$ is given by 
$$s_{XY} = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar X)(Y_i - \bar Y)$$ 
- The sample correlation coefficient $r_{XY}$ is given by
$$r_{XY} = \frac{s_{XY}}{s_{X} s_{Y}} $$ 

## Sample Correlation 
    
```{r}
inputPanel(
  sliderInput("corr", label = "Correlation:",
              min = -1, max = 1, value = 0, step = 0.1),
  selectInput("obs", label = "Observations:",
              choices = c(50, 100, 500, 10000), selected = 100)
)

renderPlot({
    d = mvrnorm(n = as.integer(input$obs), c(0, 0), matrix(c(1, input$corr, input$corr, 1), nrow = 2, ncol = 2))
    data = tibble(x=d[,1], y=d[,2])
    ggplot(data, aes(x, y)) +
        geom_point() +
        geom_smooth(method = "lm", se = FALSE) +
        scale_x_continuous(limits = c(-3,3)) +
        scale_y_continuous(limits = c(-3,3)) +
            geom_vline(xintercept = 0) + 
            geom_hline(yintercept = 0) +
        labs(x="x",y="y")
})
```

## Consistency of Sample Covariance 

- According to the law of large numbers the average of independent random variables $Y_i$ converges to the expected value $\mu_Y$
    - Requires that the variance $\sigma_Y^2$ of $Y$ is finite
- The sample covariance is also an average of independent random variables
$$s_{XY} = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar X)(Y_i - \bar Y)$$ 
    - Thus it converges in probability to the true value
    - Presupposes that it has finite variance
    - Implies finite fourth moments of $X_i$ and $Y_i$ are required

## Correlation - linear relationship
    
```{r, echo=FALSE  }
inputPanel(
  sliderInput("corr2", label = "Correlation:",
              min = -1, max = 1, value = 0, step = 0.1),
  selectInput("obs2", label = "Observations:",
              choices = c(50, 100, 500, 10000), selected = 10000)
)

renderPlot({
    d = mvrnorm(n = as.integer(input$obs2), c(0, 0), matrix(c(1, input$corr2, input$corr2, 1), nrow = 2, ncol = 2))
    data = tibble(x=d[,1], y=d[,2])
    ggplot(data, aes(x, y)) +
        geom_point() +
        geom_smooth(method = "lm", se = FALSE) +
        scale_x_continuous(limits = c(-3,3)) +
        scale_y_continuous(limits = c(-3,3)) +
            geom_vline(xintercept = 0) + 
            geom_hline(yintercept = 0) +
        labs(x="x",y="y")
})
```

## Correlation and regression

```{r , results = 'asis'}
inputPanel(
  checkboxInput(inputId = "showMean",
      label = "Means",
      value = FALSE),
  checkboxInput(inputId = "showSample",
      label = "Sample",
      value = FALSE),
  checkboxInput(inputId = "showCorr",
      label = "Correlation",
      value = FALSE),
    actionButton("updatePop", "Change")
    )

renderPlot({
terms <- reactive({
    # Change when the "update" button is pressed...
    input$updatePop
    })
terms()
points = ifelse(input$showSample, 50, 5000)
df <- tibble(X = 2.5 + .75 * rnorm(points), Y = X + 1 + .3 * rnorm(points))
m = lm(Y ~ X, df)
df$yp = predict(m)
ym=mean(df$Y)
xm=mean(df$X)
pl = df %>% 
    ggplot( aes(X, Y)) +
    geom_point() +
    xlim(0,5) + ylim(0,6) + theme_classic() +
    theme(axis.line.x = element_line(color="black"),
        axis.line.y = element_line(color="black"))

if (input$showCorr) {
    pl = pl + 
    stat_function(fun=function(x)(m$coefficients[1] + m$coefficients[2]*x), color = "red", size=1) 
}
if (input$showMean) {
    pl = pl + 
        geom_hline(yintercept = ym, linetype = 2, color="red") + 
        geom_vline(xintercept = xm, linetype = 2, color="red") 
}
pl 
})
```

## Population equation

- How does $Y$ depend on $X$?
    - Cannot hope to fully describe the relationship
- Focus on the *conditional expectation*:
    - How does the *expected value* of $Y$ depend on $X$?
- To do this, we want a function $f(X)$ such that
$$E(Y|X)=f(X)$$
- In linear models $f$ is given by the _population regression_ line
    $$E(Y|X) = \beta_{0}+\beta_{1} X$$

- $\beta_{0}$ and $\beta_{1}$ are _parameters_ in the model

## Independence and conditional probability

Consider discrete variables $X$ and $Y$  

- Independent if 
$$P(X=x,Y=y) = P(X=x) P(Y=y)$$

- Conditional probability: 
$$P(Y=y|X=x) = \frac{P(X=x,Y=y)}{P(X=x)}$$
- Thus independent if $P(Y=y|X=x) = P(Y=y)$
- Much stronger than correlation
    - Correlation is a measure of _linear_ relationships

## Some notation 

There is some more or less standard notation used in the book. If $X$ 
and $Y$ are random variables

- $\mu_X = E(X)$ - Expected value of $X$
- $\sigma^2_X = Var(X) = E((X - \mu_X)^2)$ - Variance of $X$
- $\sigma_{XY} = Cov(X, Y) = E((X - \mu_X)(Y - \mu_Y))$ - Covariance of $X$ and $Y$
- $corr(X, Y) = \rho_{XY} = \frac{\sigma_{XY} }{\sigma_{X}\sigma_{Y}}$ - Correlation coefficient of $X$ and $Y$

## Notation - population and sample{.smaller}

There is some more or less standard notation used in the book. If $X$ 
and $Y$ are random variables

Concept | Population | Sample
------------------ | ---------------- | -----------
Expected value of $X$ | $\mu_X = E(X)$ | $\bar X$
Variance of $X$ | $\sigma^2_X = Var(X)$ | $s^{2}_{X}$
Standard error of $X$ | $\sigma_X = se(X)$ | $s_{X}$
Covariance of $X$ and $Y$ | $\sigma_{XY} = Cov(X, Y)$ | $s_{XY}$
Correlation coefficient of $X$ and $Y$ | $corr(X, Y) = \rho_{XY}$ | $r_{XY}$ 

- $Y_i$ - Random draw of $Y$
- $\bar Y$ - Mean of $Y_i$

## Notation - population and sample

Population and sample definitions 

Concept | Population | Sample
------------------ | ---------------- | -----------
Expected value | $\mu_X = E(X)$ | $\bar X = \frac{1}{n}\sum_i X_i$
Variance | $\sigma^2_X = E[(X - \mu_X)^2]$ | $s^{2}_{X} = \frac{1}{n-1}\sum_i (X_i - \bar X)^2$
Covariance | $\sigma_{XY} = E[(X - \mu_X)(Y - \mu_Y)]$ | $s_{XY} = \frac{1}{n-1}\sum_i (X_i - \bar X) (Y_i - \bar Y)$
Correlation coefficient | $\rho_{XY}= \frac{\sigma_{XY} }{\sigma_{X}\sigma_{Y}}$ | $r_{XY} = \frac{s_{XY} }{s_{X} s_{Y}}$ 

## Linear regression model 

- Given a _population regression_ function
    $$E(Y|X) = \beta_{0}+\beta_{1} X$$
- A sample will consist of $n$ observations $X_{i}$ and $Y_{i}$
    - Each pair idependently and identically distributed
    - with
 $$Y_{i} = \beta_{0} + \beta_{1} X_{i} + u_{i}$$
- $u_{i}$ is the _error term_ with
$$E(u_{i}|X_{i}) = 0$$

## Mean, median and minimum distance 

- How do the mean and the median arise?
- Minimum distance = median
- Minimum square distance = mean


## Linear distance - find median

```{r }
points = 3
inputPanel(
  sliderInput("median", label = "Median:",
              min = 7, max = 11, value = 8, step = .1),
  sliderInput("high", label = "High point:",
              min = 9, max = 20, value = 11, step = .1)
)
renderPlot({
dfm <- tibble(y = c(7, 9, input$high), dens=dnorm(y, mean=5, sd=2))
    dfm <- dfm %>% 
        mutate(
            dens=abs(y - input$median)
            ) 
    lll <- paste("Distance: ", round(sum(df$dens),2))
    dfm %>% 
        ggplot( aes(y, dens)) +
        geom_point(aes(y, 0), size=3, color="blue") + 
        geom_segment(aes(xend = y, yend = 0), size = 2, lineend = "butt") +
        geom_segment(aes(xend = input$median, x = 7, y = -.25, yend = -.25), size = 2, 
            lineend = "butt", color="blue") +
        geom_segment(aes(xend = input$median, x = input$high, y = -.25, yend = -.25), size = 2, 
            lineend = "butt", color="green") +
        geom_segment(aes(xend = input$median, x = 9, y = -.5, yend = -.5), size = 2, 
            lineend = "butt", color="red") +
        stat_function(fun=function(x)(abs(x - input$median)), color="red") +
        # annotate("text", x = 8, y = 2.3, label = lll, size=7) +
        geom_point(aes(input$median, 0), size=4, color="red")
})
```

## Square distance - find mean

```{r }
points = 10
set.seed(444)
dfx <- tibble(y = 2 * rnorm(points)+5, dens=dnorm(y, mean=5, sd=2))
inputPanel(
  sliderInput("mean", label = "Mean:",
              min = 0, max = 10, value = 3, step = .1)
)
renderPlot({
    dfx <- dfx %>% 
        mutate(
            dens=(y - input$mean)^2
            ) 
    lll <- paste("Square dist: ", round(sum(df$dens),2))
    dfx %>% 
        ggplot( aes(y, dens)) +
        geom_point(aes(y, 0), size=3, color="blue") + 
        geom_segment(aes(xend = y, yend = 0), size = 2, lineend = "butt") +
        stat_function(fun=function(x)((x - input$mean)^2), color="red") +
        annotate("text", x = 7, y = -3, label = lll, size=7) +
        geom_point(aes(input$mean, 0), size=4, color="red") +
        ylim(-5, 25)
})
```

## The error term $u$ 

- For _any_ two random variables $X$ and $y$ we can write 
  $$Y = \beta_{0}+\beta_{1} X+u$$
The question is only what properties $u$ has! To see this:

- If the true relationship is nonlinear, we can rewrite:

$Y = f(X) + e$

$Y = \beta_{0} + \beta_{1} X + (f(X) + e- \beta_{0} - \beta_{1} X)$

$Y = \beta_{0} + \beta_{1} X + u$

where $u$ will depend on $X$:

$$u = f(X) + e - \beta_{0} - \beta_{1} X$$

## Ordinary Least Squares (OLS) estimation

- Given observations $X_{i}$ and $Y_{i}$
- Find $\hat\beta_{0}$ and $\hat\beta_{1}$ minimizing
$$\sum_{i=1}^n \hat u_{i}^2$$
- where the residuals $\hat u_{i}$ are defined as:
$$\hat u_{i} = Y_{i} - \hat\beta_{0} - \hat\beta_{1} X_{i}$$
- Defines the line with the minimum square distance to the observations

## Line with minimum square distance

```{r , results = 'asis', warning=FALSE }

set.seed(444)
points = 10
df2 <- tibble(X = 4 * runif(points), Y = X + 1 + 1 * rnorm(points))
ym=mean(df2$Y)

inputPanel(
  sliderInput("beta0", label = "Intercept:",
              min = 0, max = 5, value = ym, step = .1),
  sliderInput("beta1", label = "Slope:",
              min = 0, max = 2, value = 0, step = .1)
)

renderPlot({
points = 10
set.seed(444)
df <- tibble(X = 4 * runif(points), Y = X + 1 + 1 * rnorm(points))
ym=mean(df$Y)
m = lm(Y ~ X, df)
df$yp = predict(m)
ym=mean(df$Y)
xm=mean(df$X)

beta0 = input$beta0
beta1 = input$beta1
df$yp = beta0 + beta1 * df$X
df$ym = ym
df$u2 = (df$Y - df$yp)^2
txt = paste("Square err: ", round(sum(df$u2),2))    

df %>%
    ggplot( aes(X, Y)) +
    stat_function(fun=function(x)(beta0 + beta1*x), color = "red", size=1) +
    geom_point(size=3) +
    geom_hline(yintercept = ym, linetype = 2, color="red") +
    xlim(0,4) + ylim(0,6) + theme_classic() +
    geom_vline(xintercept = 0) + geom_hline(yintercept = 0) +
    geom_point(aes(X, yp), size=3, color = "blue")  +
    geom_segment(aes(xend = X, y = Y, yend = yp), size = 1, lineend = "butt", color = "red") +
    annotate("text", x = 3.5, y = 0.5, label = txt, size=7)
})

```

## Linear regression

```{r , results = 'asis'}
inputPanel(
  sliderInput("samplePoints", label = "Observations:",
              min = 10, max = 500, value = 10, step = 10),
  checkboxInput(inputId = "showMeans",
      label = "Means",
      value = FALSE),
  checkboxInput(inputId = "showTrue",
      label = "True",
      value = FALSE),
  checkboxInput(inputId = "showPredicted",
      label = "Predicted",
      value = FALSE),
    actionButton("update", "Change")
    )

renderPlot({
terms <- reactive({
    # Change when the "update" button is pressed...
    input$update
    })
terms()
points = input$samplePoints
df <- tibble(X = 4 * runif(points), Y = X + 1 + .5*rnorm(points))
m = lm(Y ~ X, df)
df$yp = predict(m)
ym=mean(df$Y)
xm=mean(df$X)
pl = df %>% 
    ggplot( aes(X, Y)) +
    stat_function(fun=function(x)(m$coefficients[1] + m$coefficients[2]*x), color = "red", size=1) +
    geom_point(size=3) +
    geom_segment(aes(xend = X, y = Y, yend = yp), size = .5, lineend = "butt", color = "red") +
    xlim(0,4) + ylim(0,5) + theme_classic() +
    geom_vline(xintercept = 0) + geom_hline(yintercept = 0) 

if (input$showPredicted) {
    pl = pl + 
    geom_point(aes(X, yp), size=3, color = "blue") 
}
if (input$showTrue) {
    pl = pl + 
    stat_function(fun=function(x)(x + 1), linetype = 2) 
}
if (input$showMeans) {
    pl = pl + 
        geom_hline(yintercept = ym, linetype = 2, color="red") + 
        geom_vline(xintercept = xm, linetype = 2, color="red") 
}
pl 
})
```

## Test scores - OLS estimation 

- Do test scores depend on student teacher ratio?
    - More studens per teacher 
    - Negative relationship
        - But much unexplained variation
- Dataset _caschool.dta_

## Test scores {.small}

```{r, echo=TRUE, comment=NA}
caschool = read_rds(url("http://bjornerstedt.org/econometrics/caschool.rds"))
reg1 = lm(testscr ~ str, data = caschool)
reg1
```

## Complete display

```{r, echo=TRUE, comment=NA}
summary(reg1)
```

## Test scores plot

```{r, echo=TRUE}
ggplot(caschool, aes(str, testscr)) + 
  geom_point() + geom_smooth(method = "lm")
```

## Employment {.tiny}

- Does wage increase with age?

```{r, echo=TRUE, comment=NA, size="tiny", eval=FALSE}
employment = read_rds(url("http://bjornerstedt.org/econometrics/employment_06_07.rds"))
reg2 = lm(earnwke ~ age, data = employment)
summary(reg2)
```

## Galton's regression

- 'Regression to the mean'
    - tall parents tend to have shorter children
    - short parents tend to have longer
- Can regress in either direction
    - tall children tend to have shorter parents
- A regression is a means of expressing correlation
    - The regressors do not **cause** the dependent variable to change!
    - No causation even if relationship is strong

## Zero conditional mean

- To ensure that we have a linear model, we assume that $E(u_{i}|X_{i}) = 0$
    - The expected value of $u_i$ does not depend on $X_i$
    - But other aspects of the distribution of $u$ could depend on $X$
- Horizontal variation $X_i$ and vertical $u_i$ are not too related

## Properties of estimators
1. _Unbiased_
    - Corresponds to population parameter in expectation
    - Both $\bar Y$ and $Y_1$ are unbiased estimates of  $\mu_Y$
2. _Consistent_
    - Converges in probability to $\mu_Y$ as sample size increases to infinity
    - $\bar Y$ but not $Y_1$ are consistent
        - Law of large numbers
    - Note that a consistent estimator can be biased
3. _Efficient_
    - Uncertainty (variance) in estimate lower than alternatives
    - $\bar Y$ efficient but not $Y_1$

## Best Linear Unbiased Estimator (BLUE)

- Unbiased: $E\left(\hat\beta_{0}\right)=\beta_{0}$ and $E\left(\hat\beta_{1}\right)=\beta_{1}$
- Consistency (no asymptotic bias)
    - Convergence (in probability) to true value as sample size $N\rightarrow\infty$
- Efficiency / best linear estimator
    - No other unbiased linear estimator has lower variance
- Unbiased variance estimate: $E\left(s^{2}\right)=\sigma^{2}$
    - Unbiased standard errors

## Predicted values

- The parameters  $\hat\beta_{0}$, $\hat\beta_{1}$ and $\hat u_i$ fit the data. For all $i$ we have
  $$ Y_{i} = \hat\beta_{0} + \hat\beta_{1} X_{i} + \hat u_i$$- The predicted value $\hat Y_{i}$ of the linear model is given by
  $$\hat Y_{i} = \hat\beta_{0} + \hat\beta_{1} X_{i}$$
- Out of sample prediction $Y$ can be obtained by inserting values of $X$ not in sample:
$$Y = \hat\beta_{0} + \hat\beta_{1} X$$

## Measures of fit
- The $R^2$ statistic is a measure of how much of the variance of $Y$ is explained by $X$
- The sample variance of $Y_{i}$ is given by
$$TSS=\sum_{i=1}^{n} (Y_{i} - \bar Y)^2$$
- The variance of the predicted values $\hat Y_{i}$ is given by
$$ESS=\sum_{i=1}^{n} (\hat Y_{i} - \bar Y)^2$$
- The $R^2$ is the ratio of the two
$$R^2 = \frac{ESS}{TSS}$$
- Standard error of the regression

## The $R^{2}$ and Test scores data
- Relatively low $R^{2}$
- Other factors affect test scores
    - Student characteristics
    - Randomness in exam results

## The Least squares assumption
- Assumption 1: Cond distribution of $u$ has mean zero
    - Relationship between $X_i$ and $u_i$ has to be specified
- Assumption 2: Observations are IID
    - $X_i$ and $Y_i$ independent of $X_j$ and $Y_j$ 
- Assumption 3: Large outliers are unlikely
    - To ensure that variance can be estimated

- Use of the OLS assumptions
    - Estimation of coefficients and their variance 
    - Unbiased and consistent estimates

## Data and regressions

- Regression with $\beta=\left(0,1\right)$, $\sigma_u^{2}=1$ and $0<x<10$
- Grey area possible linear relationships within 95% CI

![](Figures/ldisp.jpg)


## Large variance increases uncertainty

- Regression with $\sigma_u^{2}=4$


![](Figures/lvar.jpg)

## Small Variability in regressors

- Same $\beta$ and $\sigma_u^{2}$, but $4<x<6$
- Note that high slope implies small intercept

![](Figures/sdisp.jpg)

## Distribution of error $\varepsilon$ and of $\beta$

- Error term $u_i=-1,1$
- With large sample $\beta$ will be close to normal distribution
- Knowledge of distribution can increase efficiency
    - Here we can see the *exact* relationship

![](Figures/nonnormal.jpg)

## Non-normal residuals

- Residuals are gathered around $\hat u_i=-1,1$
- Not normally distributed!

![](Figures/nnresid.jpg)

# Scripting and Monte Carlo

## Variables

- Have names
    - Examples: **price emp91new empChange log_p**
- Can use national characters `√•√§√∂` 
- Can use period: `test.it`
    - Many advise against it

## Data types

- Continuous numeric
    - Different types: float, double
    - Missing values - denoted with `NA`
- Discrete numeric
    - Different types: _byte, int, long_
    - Missing values - denoted with .
    - Can have value labels
    - Dates are numeric values
- Strings
    - Data management is often converting from string
- Categorical variables / factor variables
    - Take one of a set of values (ex: yes/no/maybe)

## Monte Carlo - simulated data

- Study simulated data
- Useful to understand regression

```{r, echo=TRUE, comment=NA}    
     x = rnorm(50, 5, 2)
     u = rnorm(50, 0, 1)
     y = 2*x + 1 + u
     df = tibble(x, y, u)
     df
```

## Scatterplot of Monte Carlo data

```{r, echo=TRUE}
    ggplot(df, aes(x,y)) + geom_point()
```

## Monte Carlo - with regression line

- Scatterplot with confidence region

```{r, echo=TRUE}    
ggplot(df, aes(x,y)) + 
  geom_point() + 
  geom_smooth(method = "lm")
```


## Next lecture

- Look at Appendices 4.2 and 4.3
- Exercise
    - Calculate the covariance between y and x in Monte Carlo data
    - Study how variance of $X$ affects estimate
    
- Chapter 4 - OLS theory 
- Chapter 5 - Hypothesis tests
